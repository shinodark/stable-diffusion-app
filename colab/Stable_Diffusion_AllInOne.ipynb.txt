{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu0g2wAhokhG"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pql0vd1K5rh"
      },
      "outputs": [],
      "source": [
        "# Model on google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir CompVis\n",
        "!unzip -n /content/drive/MyDrive/stable-diffusion/stable-diffusion-v1-4.zip -d CompVis\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60R7_QWd7anA"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "# Model downloaded online => hugging Face Notebook\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "# System\n",
        "!pip install colab-xterm\n",
        "!apt install htop\n",
        "!pip install nvidia-htop\n",
        "# Models\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "!pip install diffusers transformers scipy ftfy\n",
        "!pip install \"ipywidgets>=7,<8\"\n",
        "# Upscaler\n",
        "!pip install basicsr facexlib gfpgan\n",
        "%cd Real-ESRGAN\n",
        "!pip install -r requirements.txt\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "!python setup.py develop\n",
        "%cd ..\n",
        "# ui\n",
        "!pip install streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5lqG7Kyvt3X"
      },
      "outputs": [],
      "source": [
        "# ngrok\n",
        "!ngrok authtoken personal-xxxxxxxx-auth-token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsTBsbz3vWxy"
      },
      "source": [
        "# Environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNemviouTXnG"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtrOo8YPoM2b"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JtOTsWC9kjc"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGu6cSL51aqL"
      },
      "source": [
        "# App implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN47iMXW1Z-U"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import fcntl\n",
        "import gc\n",
        "import cv2\n",
        "import time\n",
        "from torch import autocast\n",
        "import torch\n",
        "import numpy as np\n",
        "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "from tqdm.auto import tqdm\n",
        "# Upscaler\n",
        "sys.path.append(\"Real-ESRGAN\")\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from realesrgan import RealESRGANer\n",
        "from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n",
        "from gfpgan import GFPGANer\n",
        "\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDIMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    PNDMScheduler,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "# StableDiffusion Model setup\n",
        "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[DDIMScheduler, PNDMScheduler],\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        init_image: torch.FloatTensor,\n",
        "        strength: float = 0.8,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        eta: Optional[float] = 0.0,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "    ):\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if strength < 0 or strength > 1:\n",
        "          raise ValueError(f'The value of strength should in [0.0, 1.0] but is {strength}')\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        offset = 0\n",
        "        if accepts_offset:\n",
        "            offset = 1\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # encode the init image into latents and scale the latents\n",
        "        init_latents = self.vae.encode(init_image.to(self.device)).sample()\n",
        "        init_latents = 0.18215 * init_latents\n",
        "\n",
        "        # prepare init_latents noise to latents\n",
        "        init_latents = torch.cat([init_latents] * batch_size)\n",
        "        \n",
        "        # get the original timestep using init_timestep\n",
        "        init_timestep = int(num_inference_steps * strength) + offset\n",
        "        init_timestep = min(init_timestep, num_inference_steps)\n",
        "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
        "        timesteps = torch.tensor([timesteps] * batch_size, dtype=torch.long, device=self.device)\n",
        "        \n",
        "        # add noise to latents using the timesteps\n",
        "        noise = torch.randn(init_latents.shape, generator=generator, device=self.device)\n",
        "        init_latents = self.scheduler.add_noise(init_latents, noise, timesteps)\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "        # eta (Î·) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to Î· in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        latents = init_latents\n",
        "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
        "        for i, t in tqdm(enumerate(self.scheduler.timesteps[t_start:])):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents)\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image}\n",
        "\n",
        "def preprocess(image, sizemax):\n",
        "    w, h = sizemax\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h))\n",
        "    image.thumbnail(sizemax, resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def dummy_safety_checker(images, **kwargs):\n",
        "  return images, False\n",
        "\n",
        "lms = LMSDiscreteScheduler(\n",
        "    beta_start=0.00085, \n",
        "    beta_end=0.012, \n",
        "    beta_schedule=\"scaled_linear\"\n",
        ")\n",
        "\n",
        "last_pipe_type = None\n",
        "pipe = None\n",
        "def open_pipe(type=\"txt2img\"):\n",
        "  global last_pipe_type\n",
        "  global pipe\n",
        "  if last_pipe_type is not None and type == last_pipe_type:\n",
        "    return pipe\n",
        "\n",
        "  if type == \"txt2img\":\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"CompVis/stable-diffusion-v1-4\",\n",
        "        revision=\"fp16\",\n",
        "        scheduler=lms,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "  elif type == \"img2img\":\n",
        "    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "        \"CompVis/stable-diffusion-v1-4\",\n",
        "        scheduler=scheduler,\n",
        "        revision=\"fp16\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(\"cuda\")\n",
        "  \n",
        "  else:\n",
        "    print(\"Error, type have to be txt2img or img2img\")\n",
        "    return None\n",
        "\n",
        "  pipe.safety_checker = dummy_safety_checker\n",
        "  last_pipe_type = type\n",
        "  return pipe\n",
        "\n",
        "# Upscaler Model setup\n",
        "def setup_upscaler(modelname: str, scale: float, half_precision: bool):\n",
        "  if modelname == None or modelname == \"RealESRGAN_x4plus\":\n",
        "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
        "    netscale = 4\n",
        "  elif modelname in ['RealESRGAN_x4plus_anime_6B']:  # x4 RRDBNet model with 6 blocks\n",
        "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n",
        "    netscale = 4    \n",
        "  else:\n",
        "    return None\n",
        "  \n",
        "  model_path = \"Real-ESRGAN/experiments/pretrained_models/{}.pth\".format(modelname)\n",
        "  upsampler = RealESRGANer(\n",
        "    scale=netscale,\n",
        "    model_path=model_path,\n",
        "    model=model,\n",
        "    tile=0,\n",
        "    tile_pad=10,\n",
        "    pre_pad=0,\n",
        "    half=half_precision,\n",
        "    gpu_id=0)\n",
        "  \n",
        "  return upsampler\n",
        "\n",
        "# Inference function\n",
        "def infer(prompt: str, init_image_file, width: int, height: int, steps: int, cfg_scale: float, strength: float,  num_images: int, seed: int,\n",
        "          upsampling_model: str, upsampler_scale: float, upsampler_half_precision: bool, upsampler_gfpgan: bool, progress_bar):\n",
        "  \n",
        "  if seed is None or seed == 0:\n",
        "    seed = random.randrange(0,np.iinfo(np.uint32).max)\n",
        "  generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "  \n",
        "  # pipe initialisation\n",
        "  if init_image_file is not None:\n",
        "    init_image = Image.open(init_image_file)\n",
        "    init_image = preprocess(init_image, (width, height))\n",
        "    type = \"img2img\"\n",
        "  else:\n",
        "    type = \"txt2img\"  \n",
        "  pipe = open_pipe(type);\n",
        "\n",
        "  upsampler_flag = False\n",
        "  if upsampling_model != \"None\":\n",
        "    upsampler = setup_upscaler(upsampling_model, upsampler_scale, upsampler_half_precision)\n",
        "    if upsampler is not None:\n",
        "      upsampler_flag = True\n",
        "\n",
        "  # Memory cleanup\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  images_origin = []\n",
        "  images_upscaled = []\n",
        "  progress_total = num_images * (2 if upsampler_flag else 1)\n",
        "  progress_count = 0\n",
        "  with autocast(\"cuda\"):\n",
        "    for i in range(num_images):\n",
        "      if type==\"txt2img\":\n",
        "        results_origin = pipe(prompt=prompt, num_inference_steps=steps, \n",
        "                width=width, height=height, guidance_scale=cfg_scale,\n",
        "                generator=generator\n",
        "        )[\"sample\"]\n",
        "      elif type==\"img2img\":\n",
        "        results_origin = pipe( init_image=init_image, prompt=prompt, num_inference_steps=steps,\n",
        "                guidance_scale=cfg_scale, strength=strength,\n",
        "                generator=generator\n",
        "        )[\"sample\"]\n",
        "      images_origin.extend(results_origin)\n",
        "      progress_count = progress_count + len(results_origin)\n",
        "      progress_bar.progress(float(progress_count / progress_total))\n",
        "\n",
        "      if upsampler_flag:\n",
        "        for im_origin in results_origin :\n",
        "          im_origin = np.asarray(im_origin)\n",
        "          if upsampler_gfpgan:\n",
        "            face_enhancer = GFPGANer(\n",
        "              model_path=\"Real-ESRGAN/experiments/pretrained_models/GFPGANv1.3.pth\",\n",
        "              upscale=upsampler_scale,\n",
        "              arch='clean',\n",
        "              channel_multiplier=2,\n",
        "              bg_upsampler=upsampler)\n",
        "            _, _, output = face_enhancer.enhance(im_origin, has_aligned=False, only_center_face=False, paste_back=True)\n",
        "          else:\n",
        "            output, _ = upsampler.enhance(im_origin, outscale=upsampler_scale)\n",
        "          images_upscaled.append(output)\n",
        "          progress_count = progress_count + 1\n",
        "          progress_bar.progress(float(progress_count / progress_total))\n",
        "  return images_origin, images_upscaled, seed  \n",
        "\n",
        "# Streamlit GUI\n",
        "LOCK_PATH = \"/var/lock/applock\"\n",
        "\n",
        "def acquire_lock(lock_path):\n",
        "  lock_file_fd = None\n",
        "  fd = os.open(lock_path, os.O_RDWR | os.O_CREAT | os.O_TRUNC)\n",
        "  try:\n",
        "    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
        "  except (IOError, OSError):\n",
        "    pass\n",
        "  else:\n",
        "    lock_file_fd = fd\n",
        "  if lock_file_fd is None:\n",
        "    os.close(fd)\n",
        "  return lock_file_fd\n",
        "\n",
        "def release_lock(lock_file_fd):\n",
        "    fcntl.flock(lock_file_fd, fcntl.LOCK_UN)\n",
        "    os.close(lock_file_fd)\n",
        "    return None  \n",
        "\n",
        "import streamlit as st\n",
        "import streamlit.components.v1 as components\n",
        "st.set_page_config(\n",
        "     page_title=\"Stable Diffusion All in One\",\n",
        "     page_icon=\"ðŸ§Š\",\n",
        "     layout=\"wide\",\n",
        "     initial_sidebar_state=\"expanded\"\n",
        " )\n",
        "st.subheader(\"Stable Diffusion, txt2img and img2img, with upscaling (RealESRGAN and GFPGAN)\")\n",
        "with st.sidebar:\n",
        "  prompt = st.text_input(\"Prompt\", placeholder=\"concept art of a far-future city, key visual, summer day, highly detailed, digital painting, artstation, concept art, sharp focus, in harmony with nature, streamlined, by makoto shinkai and akihiko yoshida and hidari and wlop\")\n",
        "  with st.expander(\"Optional image input\"):\n",
        "    init_image = st.file_uploader(\"Initial Image\", type=['png','jpg','webp','jpeg'])\n",
        "    strength = st.slider(\"Strength (used with initial image)\",  min_value=0.0, max_value=1.0, step=0.05, value=0.75)\n",
        "  width = st.slider(\"Width\", min_value=256, max_value=768, step=64, value=512)\n",
        "  height = st.slider(\"Height\",  min_value=256, max_value=768, step=64, value=512)\n",
        "  steps = st.slider(\"Sampling Steps\",  min_value=1, max_value=150, step=1, value=50)\n",
        "  cfg_scale = st.slider(\"Classifier Free Guidance Scale\",  min_value=1.0, max_value=15.0, step=0.5, value=7.0)\n",
        "  num_images = st.slider(\"Num Images\",  min_value=1, max_value=20, step=1, value=1)\n",
        "  with st.expander(\"Upsampling\"):\n",
        "    upsampling_model = st.selectbox(\"Upsampling Model\", options=[\"None\", \"RealESRGAN_x4plus\", \"RealESRGAN_x4plus_anime_6B\"], index=0)\n",
        "    if upsampling_model != \"None\":\n",
        "      upsampler_scale = st.slider(\"Upsampler scale\",  min_value=1.0, max_value=4.0, step=0.5, value=3.0)\n",
        "      upsampler_half_precision = st.checkbox(\"Half precision\", value=False)\n",
        "      upsampler_gfpgan = st.checkbox(\"Use GFPGAN for face enhancement\", value=False)\n",
        "    else:\n",
        "      upsampler_scale = 0\n",
        "      upsampler_half_precision = False\n",
        "      upsampler_gfpgan = False\n",
        "  seed = st.number_input(\"Seed\", min_value=0)      \n",
        "  st.write(\" \")\n",
        "  st.write(\" \")\n",
        "\n",
        "with st.expander(\"Prompt build helper\"):\n",
        "  components.iframe(\"https://promptomania.com/generic-prompt-builder/\", height=600, scrolling=True)\n",
        "\n",
        "if(st.button('Run !')):\n",
        "  lock_fd = acquire_lock(LOCK_PATH)\n",
        "  if lock_fd is not None:  # prevent parallel runs because it demands too much ressources\n",
        "    with st.spinner('Generating...'):\n",
        "      progress_bar = st.progress(0)\n",
        "      prompt = prompt.replace(\"!dream \", '') # in case of a copy / paste from a !dream prompt\n",
        "      images_origin, images_upscaled, seed = infer(prompt, init_image, width, height, steps, cfg_scale, strength,  num_images, seed,\n",
        "            upsampling_model, upsampler_scale, upsampler_half_precision, upsampler_gfpgan, progress_bar)\n",
        "\n",
        "    st.info(\"Seed : \" + str(seed))\n",
        "\n",
        "    if upsampling_model != \"None\":\n",
        "      tab1, tab2 = st.columns(2)\n",
        "      with tab1:\n",
        "        st.header(\"Original images\")\n",
        "        with st.container():\n",
        "          st.image(images_origin)\n",
        "      with tab2:\n",
        "        st.header(\"Upscaled images\")\n",
        "        with st.container():\n",
        "          st.image(images_upscaled)\n",
        "    else:\n",
        "      with st.container():\n",
        "        st.image(images_origin)  \n",
        "    release_lock(lock_fd)\n",
        "  else:\n",
        "    st.warning('Busy atm, try later...', icon=\"ðŸ”¥\")\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcYCNME57Osb"
      },
      "source": [
        "# Run App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRDIKogl2zBK"
      },
      "outputs": [],
      "source": [
        "!pgrep streamlit | xargs --no-run-if-empty kill\n",
        "!nohup streamlit run /content/app.py &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Twm9z0FIBDqy"
      },
      "outputs": [],
      "source": [
        "# Connect ngork (useful one time)\n",
        "!pgrep ngrok | xargs --no-run-if-empty kill\n",
        "public_url = ngrok.connect(8501)\n",
        "display(public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCNsNdhZbsfa"
      },
      "source": [
        "# Terminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDVFlKnvw5rS"
      },
      "outputs": [],
      "source": [
        "%xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoNzDfzER8IS"
      },
      "source": [
        "# Keep active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mP9MiP3R7C-"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "while True:\n",
        "  time.sleep(10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "Yu0g2wAhokhG",
        "WsTBsbz3vWxy",
        "MoNzDfzER8IS"
      ],
      "machine_shape": "hm",
      "name": "Stable Diffusion - AllInOne",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
