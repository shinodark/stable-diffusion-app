{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinodark/stable-diffusion/blob/main/Stable_Diffusion_with_%F0%9F%A7%A8_diffuser_gradio_AllInOne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu0g2wAhokhG"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pql0vd1K5rh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "#!pip install diffusers==0.2.4\n",
        "!pip install git+https://github.com/huggingface/diffusers.git@main#egg=diffusers\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "!pip install transformers scipy ftfy gradio\n",
        "!pip install \"ipywidgets>=7,<8\"\n",
        "!pip install basicsr facexlib gfpgan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyRaOMr4LbCN"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/stable-diffusion/stable-diffusion-v1-4.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO8MsPQXG4Mv"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount() "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upscaler\n",
        "%cd Real-ESRGAN\n",
        "!pip install -r requirements.txt\n",
        "!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n",
        "!python setup.py develop\n",
        "%cd .."
      ],
      "metadata": {
        "id": "MiVMaiCbsCm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsTBsbz3vWxy"
      },
      "source": [
        "# Environnement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "TNemviouTXnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtrOo8YPoM2b"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZzX55QnwTMX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import sys\n",
        "import gc\n",
        "import cv2\n",
        "import time\n",
        "from torch import autocast\n",
        "import torch\n",
        "import numpy as np\n",
        "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "import gradio as gr\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "from tqdm.auto import tqdm\n",
        "# Upscaler\n",
        "sys.path.append(\"Real-ESRGAN\")\n",
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from realesrgan import RealESRGANer\n",
        "from realesrgan.archs.srvgg_arch import SRVGGNetCompact\n",
        "from gfpgan import GFPGANer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc4VNGjqswop"
      },
      "source": [
        "# Model initialisation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stable Diffusion Model"
      ],
      "metadata": {
        "id": "eUpCe3cpsXHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDIMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    PNDMScheduler,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[DDIMScheduler, PNDMScheduler],\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        init_image: torch.FloatTensor,\n",
        "        strength: float = 0.8,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        eta: Optional[float] = 0.0,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "    ):\n",
        "\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if strength < 0 or strength > 1:\n",
        "          raise ValueError(f'The value of strength should in [0.0, 1.0] but is {strength}')\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        offset = 0\n",
        "        if accepts_offset:\n",
        "            offset = 1\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # encode the init image into latents and scale the latents\n",
        "        init_latents = self.vae.encode(init_image.to(self.device)).sample()\n",
        "        init_latents = 0.18215 * init_latents\n",
        "\n",
        "        # prepare init_latents noise to latents\n",
        "        init_latents = torch.cat([init_latents] * batch_size)\n",
        "        \n",
        "        # get the original timestep using init_timestep\n",
        "        init_timestep = int(num_inference_steps * strength) + offset\n",
        "        init_timestep = min(init_timestep, num_inference_steps)\n",
        "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
        "        timesteps = torch.tensor([timesteps] * batch_size, dtype=torch.long, device=self.device)\n",
        "        \n",
        "        # add noise to latents using the timesteps\n",
        "        noise = torch.randn(init_latents.shape, generator=generator, device=self.device)\n",
        "        init_latents = self.scheduler.add_noise(init_latents, noise, timesteps)\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "        # eta (Î·) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to Î· in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        latents = init_latents\n",
        "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
        "        for i, t in tqdm(enumerate(self.scheduler.timesteps[t_start:])):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents)\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image}"
      ],
      "metadata": {
        "id": "1EHjt5uzFlCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image, sizemax):\n",
        "    w, h = sizemax\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h))\n",
        "    image.thumbnail(sizemax, resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def dummy_safety_checker(images, **kwargs):\n",
        "  return images, False"
      ],
      "metadata": {
        "id": "F2cI-J8v7Q66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [],
      "source": [
        "lms = LMSDiscreteScheduler(\n",
        "    beta_start=0.00085, \n",
        "    beta_end=0.012, \n",
        "    beta_schedule=\"scaled_linear\"\n",
        ")\n",
        "\n",
        "last_pipe_type = None\n",
        "pipe = None\n",
        "def open_pipe(type=\"txt2img\"):\n",
        "  global last_pipe_type\n",
        "  global pipe\n",
        "  if last_pipe_type is not None and type == last_pipe_type:\n",
        "    return pipe\n",
        "\n",
        "  if type == \"txt2img\":\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"stable-diffusion-v1-4\",\n",
        "        revision=\"fp16\",\n",
        "        scheduler=lms,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "  elif type == \"img2img\":\n",
        "    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "        \"stable-diffusion-v1-4\",\n",
        "        scheduler=scheduler,\n",
        "        revision=\"fp16\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(\"cuda\")\n",
        "  \n",
        "  else:\n",
        "    print(\"Error, type have to be txt2img or img2img\")\n",
        "    return None\n",
        "\n",
        "  pipe.safety_checker = dummy_safety_checker\n",
        "  last_pipe_type = type\n",
        "  return pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upscaler Model"
      ],
      "metadata": {
        "id": "hTCkq9L5sewU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_upscaler(modelname: str, scale: float, half_precision: bool):\n",
        "  if modelname == None or modelname == \"RealESRGAN_x4plus\":\n",
        "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
        "    netscale = 4\n",
        "  else:\n",
        "    return None\n",
        "  \n",
        "  upsampler = RealESRGANer(\n",
        "    scale=netscale,\n",
        "    model_path='Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth',\n",
        "    model=model,\n",
        "    tile=0,\n",
        "    tile_pad=10,\n",
        "    pre_pad=0,\n",
        "    half=half_precision,\n",
        "    gpu_id=0)\n",
        "  \n",
        "  return upsampler"
      ],
      "metadata": {
        "id": "idPxSeNpsge0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCNsNdhZbsfa"
      },
      "source": [
        "# Gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(prompt: str, init_image, width: int, height: int, steps: int, cfg_scale: float, strength: float,  num_images: int, seed: int,\n",
        "          upsampling_model: str, upsampler_scale: float, upsampler_half_precision: bool, upsampler_gfpgan: bool):\n",
        "  if seed is None or seed == 0:\n",
        "    seed = random.randrange(0,np.iinfo(np.uint32).max)\n",
        "  generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "  \n",
        "  # pipe initialisation\n",
        "  if init_image is not None:\n",
        "    init_image = preprocess(init_image, (width, height))\n",
        "    type = \"img2img\"\n",
        "  else:\n",
        "    type = \"txt2img\"  \n",
        "  pipe = open_pipe(type);\n",
        "\n",
        "  upsampler_flag = False\n",
        "  if upsampling_model != \"None\":\n",
        "    upsampler = setup_upscaler(upsampling_model, upsampler_scale, upsampler_half_precision)\n",
        "    if upsampler is not None:\n",
        "      upsampler_flag = True\n",
        "\n",
        "  # Memory cleanup\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  images_origin = []\n",
        "  images_upscaled = []\n",
        "  with autocast(\"cuda\"):\n",
        "    for i in range(num_images):\n",
        "      if type==\"txt2img\":\n",
        "        results_origin = pipe(prompt=prompt, num_inference_steps=steps, \n",
        "                width=width, height=height, guidance_scale=cfg_scale,\n",
        "                generator=generator\n",
        "        )[\"sample\"]\n",
        "      elif type==\"img2img\":\n",
        "        results_origin = pipe( init_image=init_image, prompt=prompt, num_inference_steps=steps,\n",
        "                guidance_scale=cfg_scale, strength=strength,\n",
        "                generator=generator\n",
        "        )[\"sample\"]\n",
        "      \n",
        "      if upsampler_flag:\n",
        "        for im_origin in results_origin :\n",
        "          im_origin = np.asarray(im_origin)\n",
        "          if upsampler_gfpgan:\n",
        "            face_enhancer = GFPGANer(\n",
        "              model_path='Real-ESRGAN/experiments/pretrained_models/GFPGANv1.3.pth',\n",
        "              upscale=upsampler_scale,\n",
        "              arch='clean',\n",
        "              channel_multiplier=2,\n",
        "              bg_upsampler=upsampler)\n",
        "            _, _, output = face_enhancer.enhance(im_origin, has_aligned=False, only_center_face=False, paste_back=True)\n",
        "          else:\n",
        "            output, _ = upsampler.enhance(im_origin, outscale=upsampler_scale)\n",
        "          \n",
        "          images_upscaled.append(PIL.Image.fromarray(output))\n",
        "      images_origin.extend(results_origin)\n",
        "\n",
        "  print(\"Request processed, upsale : \", upsampler_flag)  \n",
        "  \n",
        "  return images_origin, images_upscaled, seed"
      ],
      "metadata": {
        "id": "2osgmyfM4pAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUQ3DqFjd_17"
      },
      "outputs": [],
      "source": [
        "interface = gr.Interface(\n",
        "    infer,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Prompt\", lines=4, placeholder=\"concept art of a far-future city, key visual, summer day, highly detailed, digital painting, artstation, concept art, sharp focus, in harmony with nature, streamlined, by makoto shinkai and akihiko yoshida and hidari and wlop\"),\n",
        "        gr.Image(label=\"Initial Image\", type=\"pil\"),\n",
        "        gr.Slider(minimum=64, maximum=768, step=64, label=\"Width\", value=512),\n",
        "        gr.Slider(minimum=64, maximum=768, step=64, label=\"Height\", value=512),\n",
        "        gr.Slider(minimum=1, maximum=150, step=1, label=\"Sampling Steps\", value=50),\n",
        "        gr.Slider(minimum=1.0, maximum=15.0, step=0.5, label='Classifier Free Guidance Scale', value=7.0),\n",
        "        gr.Slider(minimum=0.0, maximum=1.0, step=0.05, label='Strength (used with initial image)', value=0.75),\n",
        "        gr.Slider(label='Num Images', minimum=1, maximum=20, step=1),\n",
        "        gr.Number(label='Seed', precision=0, value=0),\n",
        "        gr.Dropdown([\"None\", \"RealESRGAN_x4plus\"], label=\"Upsampling Model\", value=\"None\"),\n",
        "        gr.Slider(minimum=1, maximum=4, step=0.5, label=\"Upsampler scale\", value=2),\n",
        "        gr.Checkbox(label=\"Half precision\", value=False),\n",
        "        gr.Checkbox(label=\"Use GFPGAN for face enhancement\", value=False)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Gallery(label=\"Image (original)\", show_label=False),\n",
        "        gr.Gallery(label=\"Image (upscaled)\", show_label=False),\n",
        "        gr.Number(label='Seed'),\n",
        "    ],\n",
        "    title=\"Stable Diffusion with upscaling (RealESRGAN and GFPGAN)\",\n",
        "    description=\"BoÃ®te Ã  sable Stable Diffusion (K-LMS)\",\n",
        "    allow_flagging=\"never\")\n",
        "interface.launch(max_threads=1, debug=False, prevent_thread_lock=True, inline=False, inbrowser=True, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WsTBsbz3vWxy"
      ],
      "machine_shape": "hm",
      "name": "Stable Diffusion with ðŸ§¨ diffuser - gradio - AllInOne",
      "provenance": [],
      "private_outputs": true,
      "background_execution": "on",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}